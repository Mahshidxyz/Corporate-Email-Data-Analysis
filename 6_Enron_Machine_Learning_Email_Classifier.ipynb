{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Email Classification\n",
    "\n",
    "\n",
    "Each user had a chance to classify their emails and put them in labeled folders. Herein I built a classifier for emails owned by a single employee based on the labels that they used to classify their own emails. The classifier uses the subject and content to train the model and uses the Multinomial Naive Bayes algorithm. \n",
    "\n",
    "I chose to work with the emails owned by user 'kaminski-v'. Vince Kaminski is the top email user in the database who sent and received 28465 emails. One of their multiple email accounts \"vince.kaminski@enron.com\" has sent 14368 emails which is the second most among all users. It seems that they labeled his emails better than the rest of the top users and that's why I chose them. \n",
    "\n",
    "I start with the cleaned data that I prepared in the previous sections of the project. \n",
    "\n",
    "\n",
    "- **Part 1: Extracting labels from data **\n",
    "- **Part 2: Tokenization & Cleaning **\n",
    "- **Part 3: Machine Learning: Multinomial Naive Bayes, n-gram Naive Bayes, Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenization and cleaning \n",
    "import re\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Machine Learning: Bag of words, Multinomial Naive Bayes \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TF-IDF, Logistic Regression and n-gram will be imported on the spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bcc</th>\n",
       "      <th>Cc</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>Subject</th>\n",
       "      <th>To</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-From</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-To</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>content</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message-ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-05-14 23:39:00</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>Tim Belden &lt;Tim Belden/Enron@EnronXGate&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is our forecast\\r\\n\\r\\n</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-05-04 20:51:00</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>Re:</td>\n",
       "      <td>john.lavorato@enron.com</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>John J Lavorato &lt;John J Lavorato/ENRON@enronXg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;24216240.1075855687451.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-10-18 10:00:00</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>Re: test</td>\n",
       "      <td>leah.arsdall@enron.com</td>\n",
       "      <td>pallen.nsf</td>\n",
       "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>Leah Van Arsdall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;13505866.1075863688222.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-10-23 13:13:00</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>randall.gay@enron.com</td>\n",
       "      <td>pallen.nsf</td>\n",
       "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>Randall L Gay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randy,\\r\\n\\r\\n Can you send me a schedule of t...</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;30922949.1075863688243.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000-08-31 12:07:00</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>Re: Hello</td>\n",
       "      <td>greg.piper@enron.com</td>\n",
       "      <td>pallen.nsf</td>\n",
       "      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>Greg Piper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "      <td>allen-p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Bcc   Cc                 Date  \\\n",
       "Message-ID                                                                     \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>  NaN  NaN  2001-05-14 23:39:00   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  NaN  NaN  2001-05-04 20:51:00   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>  NaN  NaN  2000-10-18 10:00:00   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>  NaN  NaN  2000-10-23 13:13:00   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>  NaN  NaN  2000-08-31 12:07:00   \n",
       "\n",
       "                                                                  From  \\\n",
       "Message-ID                                                               \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>  phillip.allen@enron.com   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  phillip.allen@enron.com   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>  phillip.allen@enron.com   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>  phillip.allen@enron.com   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>  phillip.allen@enron.com   \n",
       "\n",
       "                                                 Subject  \\\n",
       "Message-ID                                                 \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>        NaN   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>        Re:   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>   Re: test   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>        NaN   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>  Re: Hello   \n",
       "\n",
       "                                                                    To  \\\n",
       "Message-ID                                                               \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>     tim.belden@enron.com   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  john.lavorato@enron.com   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>   leah.arsdall@enron.com   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>    randall.gay@enron.com   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>     greg.piper@enron.com   \n",
       "\n",
       "                                                                X-FileName  \\\n",
       "Message-ID                                                                   \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>  pallen (Non-Privileged).pst   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  pallen (Non-Privileged).pst   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>                   pallen.nsf   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>                   pallen.nsf   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>                   pallen.nsf   \n",
       "\n",
       "                                                                                        X-Folder  \\\n",
       "Message-ID                                                                                         \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>    \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>    \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>    \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail   \n",
       "\n",
       "                                                        X-From X-Origin  \\\n",
       "Message-ID                                                                \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>  Phillip K Allen  Allen-P   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  Phillip K Allen  Allen-P   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>  Phillip K Allen  Allen-P   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>  Phillip K Allen  Allen-P   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>  Phillip K Allen  Allen-P   \n",
       "\n",
       "                                                                                            X-To  \\\n",
       "Message-ID                                                                                         \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>           Tim Belden <Tim Belden/Enron@EnronXGate>   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  John J Lavorato <John J Lavorato/ENRON@enronXg...   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>                                   Leah Van Arsdall   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>                                      Randall L Gay   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>                                         Greg Piper   \n",
       "\n",
       "                                              X-bcc X-cc  \\\n",
       "Message-ID                                                 \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>   NaN  NaN   \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>   NaN  NaN   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>   NaN  NaN   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>   NaN  NaN   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>   NaN  NaN   \n",
       "\n",
       "                                                                                         content  \\\n",
       "Message-ID                                                                                         \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>                      Here is our forecast\\r\\n\\r\\n    \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  Traveling to have a business meeting takes the...   \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>                     test successful.  way to go!!!   \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>  Randy,\\r\\n\\r\\n Can you send me a schedule of t...   \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>                Let's shoot for Tuesday at 11:45.     \n",
       "\n",
       "                                                  user  \n",
       "Message-ID                                              \n",
       "<18782981.1075855378110.JavaMail.evans@thyme>  allen-p  \n",
       "<15464986.1075855378456.JavaMail.evans@thyme>  allen-p  \n",
       "<24216240.1075855687451.JavaMail.evans@thyme>  allen-p  \n",
       "<13505866.1075863688222.JavaMail.evans@thyme>  allen-p  \n",
       "<30922949.1075863688243.JavaMail.evans@thyme>  allen-p  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the preprocessed data frame\n",
    "df = pd.read_csv('out.csv', index_col='Message-ID', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Cleaning & Wrangling - Extracting Labels\n",
    "\n",
    "- Building a new dataframe from emails sent and received by the top email user 'kaminski-v' (Vince Kaminski)\n",
    "- Extracting not too general labels with about 200 emails or more.\n",
    "- Removing the unwanted columns from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails sent or received by top email user (kaminski-v): 28465\n"
     ]
    }
   ],
   "source": [
    "df_ml = df[df['user'] == 'kaminski-v']\n",
    "print('Number of emails sent or received by top email user (kaminski-v):', len(df_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Folder\n",
      "\\Vincent_Kaminski_Jun2001_1\\Notes Folders\\All documents                 5066\n",
      "\\Vincent_Kaminski_Jun2001_2\\Notes Folders\\Discussion threads            3980\n",
      "\\Vincent_Kaminski_Jun2001_4\\Notes Folders\\'sent mail                    2574\n",
      "\\Vincent_Kaminski_Jun2001_3\\Notes Folders\\Sent                          2573\n",
      "\\Vincent_Kaminski_Jun2001_6\\Notes Folders\\All documents                 2108\n",
      "\\Vincent_Kaminski_Jun2001_7\\Notes Folders\\Discussion threads            1570\n",
      "\\Vincent_Kaminski_Jun2001_8\\Notes Folders\\'sent mail                     890\n",
      "\\Vincent_Kaminski_Jun2001_8\\Notes Folders\\Sent                           890\n",
      "\\VKAMINS (Non-Privileged)\\Kaminski, Vince J\\Sent Items                   827\n",
      "\\vkamins\\Deleted Items                                                   691\n",
      "\\VKAMINS (Non-Privileged)\\Kaminski, Vince J\\Deleted Items                572\n",
      "\\Vince_Kaminski_Jun2001_10\\Sent Items                                    498\n",
      "\\Vincent_Kaminski_Jun2001_5\\Notes Folders\\C:\\Mangmt\\Group\\Management     366\n",
      "\\Vincent_Kaminski_Jun2001_5\\Notes Folders\\Resumes                        365\n",
      "\\Vincent_Kaminski_Jun2001_5\\Notes Folders\\Universities                   363\n",
      "\\Vince_Kaminski_Jun2001_10\\Deleted Items                                 318\n",
      "\\vkamins\\Sent Items                                                      314\n",
      "\\Vincent_Kaminski_Jun2001_5\\Notes Folders\\C:\\Technote\\Mail\\Projects      289\n",
      "\\VKAMINS (Non-Privileged)\\Kaminski, Vince J\\Inbox                        237\n",
      "\\Vincent_Kaminski_Jan2002_1\\Kaminski, Vince J\\Deleted Items              211\n",
      "Name: X-Folder, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The X-Folder directiries (header) which we will use for labeling\n",
    "print(df_ml.groupby('X-Folder')['X-Folder'].count().sort_values(ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mahshid\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "c:\\users\\mahshid\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "c:\\users\\mahshid\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# exctracting the final folder name from the directory\n",
    "# split does not work on \\ (escape character, string literal) so I replaced \\ with \\\\\n",
    "df_ml.loc[:, 'X-Folder'] = df_ml['X-Folder'].astype(str) # some of the folders were float\n",
    "df_ml.loc[:,'labels'] = df_ml['X-Folder'].map(lambda x: x.lower().split('\\\\')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "all documents         7174\n",
      "discussion threads    5550\n",
      "'sent mail            3464\n",
      "sent                  3463\n",
      "deleted items         1792\n",
      "sent items            1696\n",
      "management             802\n",
      "inbox                  560\n",
      "resumes                547\n",
      "projects               379\n",
      "universities           367\n",
      "personal               281\n",
      "ene_ect                270\n",
      "conferences            223\n",
      "notes inbox            223\n",
      "london                 195\n",
      "techmemos              187\n",
      "rice                   175\n",
      "australia              112\n",
      "var                    110\n",
      "eci                    109\n",
      "ut                      76\n",
      "stanford                76\n",
      "credit                  65\n",
      "calendar                60\n",
      "evaluation              57\n",
      "risk                    54\n",
      "ei                      51\n",
      "consultants             44\n",
      "sites                   30\n",
      "cera                    28\n",
      "esai                    25\n",
      "rac                     24\n",
      "ees                     23\n",
      "weather                 21\n",
      "presentations           21\n",
      "gpg                     19\n",
      "ene/ect                 18\n",
      "pjm                     17\n",
      "newsletter              14\n",
      "poland                   9\n",
      "untitledmartin           8\n",
      "uh                       7\n",
      "audit                    7\n",
      "technical                7\n",
      "fasb133                  6\n",
      "duffie                   5\n",
      "publications             4\n",
      "azurix                   3\n",
      "ferc                     3\n",
      "nan                      2\n",
      "tasks                    1\n",
      "accounting               1\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_ml.groupby('labels')['labels'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical labels: ['management', 'resumes', 'universities', 'projects', 'conferences', 'london', 'ene_ect']\n"
     ]
    }
   ],
   "source": [
    "# labels with about 200 emails or more that are not too general.\n",
    "# I did not select Personal because in contains lot of emails in Polish \n",
    "labels_to_keep = ['management', 'resumes', 'universities',\\\n",
    "                  'projects', 'conferences', 'london', 'ene_ect']\n",
    "print('Categorical labels:', labels_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows with unwanted labels\n",
    "df_ml = df_ml[df_ml['labels'].isin(labels_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "management      802\n",
      "resumes         547\n",
      "projects        379\n",
      "universities    367\n",
      "ene_ect         270\n",
      "conferences     223\n",
      "london          195\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_ml.groupby('labels')['labels'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to avoid having unbalanced training data and putting weight on more frequent labels I reduce the size of different categories to the minimum size which is 195 samples for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling to have equal size categories\n",
    "df_ml_r = pd.DataFrame()\n",
    "for lab in labels_to_keep:\n",
    "    df_ml_r = pd.concat([df_ml_r, df_ml.loc[df_ml['labels'] == lab, :].sample(195, random_state=42)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "universities    195\n",
      "resumes         195\n",
      "projects        195\n",
      "management      195\n",
      "london          195\n",
      "ene_ect         195\n",
      "conferences     195\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the sampled data\n",
    "df_ml = df_ml_r\n",
    "del df_ml_r\n",
    "print(df_ml.groupby('labels')['labels'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column for subject + content\n",
    "# some of the emails have no subject line. Without filling the nan the result of the addition becomes null.\n",
    "df_ml['Subject'] = df_ml['Subject'].fillna('')\n",
    "df_ml['text'] = df_ml['Subject'] + '\\n' + df_ml['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1365 entries, 0 to 1364\n",
      "Data columns (total 5 columns):\n",
      "Message-ID    1365 non-null object\n",
      "labels        1365 non-null object\n",
      "Subject       1365 non-null object\n",
      "content       1365 non-null object\n",
      "text          1365 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 53.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# removing the extra columns\n",
    "df_ml = df_ml[['labels', 'Subject', 'content', 'text']].reset_index()\n",
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By focusing on the not too general labels with 195 data points each, we end up with only 1365 emails for classification.\n",
    "\n",
    "## Part 2: Tokenization & Cleaning\n",
    "- Removing numerical characters from the text \n",
    "- Tokenization (list of tokens)\n",
    "- Removing stop words\n",
    "- Removing \" _ \" used as empty spaces to fill in present in the emails\n",
    "- Concatanating the list of tokens to make a string for CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing digits\n",
    "df_ml['text_nonum'] = df_ml['text'].map(lambda x: re.sub(r'\\d+', '',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing to remove stop words. This converts the text to a list of tokens\n",
    "tokenizer = RegexpTokenizer(r'(?u)\\b\\w\\w+\\b')\n",
    "df_ml['text_token'] = df_ml['text_nonum'].map(lambda x: tokenizer.tokenize(x)) # class of each element: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the stopwords and also lowercasing\n",
    "df_ml['text_token'] = df_ml['text_token'].map(lambda x: [word.lower() for word in x if word not in (ENGLISH_STOP_WORDS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing some common words in emails\n",
    "words_to_go = ['to', 'from', 'am', 'pm']\n",
    "df_ml['text_token'] = df_ml['text_token'].map(lambda x: [word for word in x if word not in (words_to_go)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove ____... from the text\n",
    "df_ml['text_token'] = df_ml['text_token'].map(lambda x: [word.replace('_', ' ') for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a string from a list of tokens so that it can work with CountVectorizer\n",
    "df_ml['text_str'] = df_ml['text_token'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Machine Learning \n",
    "\n",
    "I used the bag of words model for feature extraction from text.\n",
    "\n",
    "In the bag of words model, each document is treated as a vector. Each element in the vector contains some kind of data about the words that appear in the document such as presence/absence (1/0), count (an integer) or some other statistic. Each vector has the same length because each document shared the same vocabulary across the full collection of documents. This collection is called a corpus. Then, a set of documents becomes, in the usual sklearn style, a sparse matrix with rows being sparse arrays representing documents and columns representing the features/words in the vocabulary. Notice that the bag of words treatment doesn't preserve information about the order of words, just their frequency.\n",
    "\n",
    "For classification I used Multinomial Naive Bayes with two different methods for vectorization, n-gram naive bayes, and logistic regression. I used cross validation to tune the hyperparameters for all of the models.\n",
    "\n",
    "### 3.1. Multinomial Naive Bayes\n",
    "\n",
    "Naive Bayes classifier is a general term which refers to conditional independence of each of the features in the model, while Multinomial Naive Bayes classifier is a specific instance of a Naive Bayes classifier which uses a multinomial distribution for each of the features. \n",
    "\n",
    "The main assumption of Naive Bayes is that the features are conditionally independent given the class. While the presence of a particular discriminative word may uniquely identify the document as being part of a particular class and thus violate general feature independence, conditional independence means that the presence of that term is independent of all the other words that appear within that class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 features: \n",
      " ['aa', 'aadedeji', 'aaldous', 'aanalysis', 'aaro', 'ab', 'abahy', 'abb', 'abdul', 'abdullah', 'abhay', 'abilit', 'abilities', 'ability', 'abitibi', 'able', 'abler', 'abliged', 'abn', 'abo', 'aboard', 'aboriginal', 'abou', 'about', 'above', 'abrams', 'abreast', 'abridged', 'abroad', 'abs', 'absence', 'absolutely', 'absorb', 'absorbed', 'absorbing', 'abstract', 'abstracts', 'abu', 'abundance', 'abuse', 'ac', 'aca', 'academe', 'academia', 'academic', 'academically', 'academics', 'academy', 'acadrep', 'acc', 'accelerate', 'accelerated', 'accelerating', 'acceleration', 'accenture', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts', 'acces', 'access', 'accessenergy', 'accessibility', 'accessible', 'accessing', 'accident', 'acclaimed', 'accommodate', 'accommodates', 'accommodating', 'accommodation', 'accommodations', 'accomodate', 'accomodating', 'accomodation', 'accomodative', 'accompanied', 'accompanies', 'accompany', 'accomplish', 'accomplished', 'accomplishemnts', 'accomplishing', 'accomplishment', 'accomplishments', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountable', 'accountants', 'accounted', 'accountin', 'accounting', 'accounts', 'accreditation']\n",
      "...\n",
      "Last 100 features: \n",
      " ['yd', 'ye', 'yea', 'yeager', 'year', 'yearend', 'years', 'yeboah', 'yeck', 'yen', 'yeow', 'yergin', 'yes', 'yesterday', 'yesterdays', 'yet', 'yg', 'yh', 'yield', 'yieldin', 'yields', 'yin', 'ying', 'yingquan', 'yip', 'yn', 'yo', 'yoho', 'yolanda', 'yomogida', 'yopu', 'yoram', 'york', 'you', 'young', 'youngblood', 'your', 'yours', 'youth', 'youthful', 'youyi', 'yowman', 'yoyukrpi', 'ypur', 'yr', 'yrs', 'ytta', 'yu', 'yuan', 'yumi', 'yvan', 'yvette', 'yyyymmdd', 'zadorozhny', 'zak', 'zamin', 'zar', 'zargon', 'zariphopoulou', 'zb', 'zc', 'zd', 'ze', 'zealand', 'zeebrugge', 'zehn', 'zeitraum', 'zero', 'zhan', 'zhang', 'zhendong', 'zhimin', 'zhiyong', 'zimin', 'zimmerman', 'zip', 'ziplip', 'zipped', 'zipper', 'zipter', 'zisman', 'zlu', 'zmtmehikzukdqxamxdragpwwduhooegkzircvltmipedxe', 'zn', 'zoch', 'zoe', 'zoko', 'zonal', 'zone', 'zu', 'zufferli', 'zulie', 'zulkifli', 'zum', 'zunaechst', 'zur', 'zusaetzlich', 'zuzana', 'zwischen', 'zzmacmac']\n"
     ]
    }
   ],
   "source": [
    "# Making the Bag of Words and the labels!\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(df_ml.text_str)\n",
    "X = vectorizer.transform(df_ml.text_str)\n",
    "print('First 100 features: \\n', vectorizer.get_feature_names() [:100])\n",
    "print ('...')\n",
    "print('Last 100 features: \\n', vectorizer.get_feature_names() [-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13676\n"
     ]
    }
   ],
   "source": [
    "print('Number of features:', len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the sparse matrix:  (1365, 13676)\n",
      "Non-Zero occurrences:  124627\n",
      "Percentage of non-zero values: % 0.6676062554974518\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the sparse matrix: ', X.shape)\n",
    "print('Non-Zero occurrences: ', X.nnz)\n",
    "# Percentage of non-zero values\n",
    "density = (100.0 * X.nnz / (X.shape[0] * X.shape[1]))\n",
    "print('Percentage of non-zero values: %', density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### So far\n",
    "- 1365 emails labeled in 8 different categories are vectorized.\n",
    "- There are 13676 features, with min-df = 1 (keepeing words with frequency >= 1) .\n",
    "- Some of the words in this bag are in Polish, not English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y labels\n",
    "y = df_ml['labels'].values # df_ml.labels.values would not work, '.labels' might be considered an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Training Set (MNB): 0.9230769230769231\n",
      "Accuracy Score for Test Set (MNB): 0.7289377289377289\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data for training and test\n",
    "Xlr, Xtest, ylr, ytest = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# Naive Bayes Multinomial\n",
    "MNB = MultinomialNB(alpha=1) # cross-validation will be performed later\n",
    "MNB.fit(Xlr, ylr)\n",
    "print('Accuracy Score for Training Set (MNB): {}'.format(accuracy_score(ylr, MNB.predict(Xlr))))\n",
    "print('Accuracy Score for Test Set (MNB): {}'.format(accuracy_score(ytest, MNB.predict(Xtest))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = MNB.predict(X)\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of classes:\n",
      " ['conferences' 'ene_ect' 'london' 'management' 'projects' 'resumes'\n",
      " 'universities']\n",
      "Number of samples encountered for each class during training:\n",
      "[ 153.  147.  154.  162.  159.  162.  155.]\n"
     ]
    }
   ],
   "source": [
    "# name of the classes, see dir(MNB) to see available methods\n",
    "print ('Name of classes:\\n', MNB.classes_)\n",
    "print('Number of samples encountered for each class during training:')\n",
    "print(MNB.class_count_) # in the training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation to tune alpha (regularization parameter) of MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned alpha parameter: {'alpha': 1}\n",
      "Mean cross-validated accuracy of tuned model for the hold out data: 0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "# Tune the hyperparameter of MNB using GridSearchCV\n",
    "\n",
    "alpha_space = [1, 1.5, 2, 5, 10, 50]\n",
    "param_grid = {'alpha': alpha_space}\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "mnb_cv = GridSearchCV(mnb, param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "mnb_cv.fit(X_train,y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "# Parameter setting that gave the best results on the hold out data\n",
    "print(\"Tuned alpha parameter: {}\".format(mnb_cv.best_params_)) \n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"Mean cross-validated accuracy of tuned model for the hold out data: {}\".format(mnb_cv.best_score_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Training Set (MNB-CV): 0.9230769230769231\n",
      "Accuracy Score for Test Set (MNB-CV): 0.7289377289377289\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy using the best alpha which turned out the be equal to the default alpha = 1 \n",
    "print('Accuracy Score for Training Set (MNB-CV): {}'.format(accuracy_score(y_train, mnb_cv.predict(X_train))))\n",
    "print('Accuracy Score for Test Set (MNB-CV): {}'.format(accuracy_score(y_test, mnb_cv.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try another way of vectorization instead of Count Vectorizer called TF-IDF vectorizer. TF-IDF stands for “term frequency / inverse document frequency” and is a method for emphasizing words that occur frequently in a given document, while at the same time de-emphasising words that occur frequently in many documents.\n",
    "\n",
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13479\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighting instead of word counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(min_df=1, stop_words='english') # some stop words have been previously removed\n",
    "Xtfidf = tfidfvectorizer.fit_transform(df_ml.text_str)\n",
    "print('Number of features:', len(tfidfvectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned alpha parameter: {'alpha': 1}\n",
      "Mean cross-validated accuracy of tuned model for the hold out data: 0.7490842490842491\n"
     ]
    }
   ],
   "source": [
    "# MNB with TF-IDF weighting instead of word counts + Cross Validation\n",
    "\n",
    "mnb_tfidf = MultinomialNB()\n",
    "alpha_space = [1, 1.5, 2, 5, 10, 50]\n",
    "param_grid = {'alpha': alpha_space}\n",
    "\n",
    "\n",
    "# Create train and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(Xtfidf, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "mnb_cv_tfidf = GridSearchCV(mnb_tfidf, param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "mnb_cv_tfidf.fit(X_train_tfidf,y_train_tfidf)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "# Parameter setting that gave the best results on the hold out data\n",
    "print(\"Tuned alpha parameter: {}\".format(mnb_cv_tfidf.best_params_)) \n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"Mean cross-validated accuracy of tuned model for the hold out data: {}\".format(mnb_cv_tfidf.best_score_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Training Set (MNB-TFIDF-CV): 0.9276556776556777\n",
      "Accuracy Score for Test Set (MNB-TFIDF-CV): 0.7472527472527473\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy using the best alpha which turned out to be same as the default alpha = 1 \n",
    "print('Accuracy Score for Training Set (MNB-TFIDF-CV): {}'\\\n",
    "      .format(accuracy_score(y_train_tfidf, mnb_cv_tfidf.predict(X_train_tfidf))))\n",
    "print('Accuracy Score for Test Set (MNB-TFIDF-CV): {}'\\\n",
    "      .format(accuracy_score(y_test_tfidf, mnb_cv_tfidf.predict(X_test_tfidf))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. n-gram Feature Multinomial Naive Bayes\n",
    "\n",
    "N-grams are phrases containing n words next to each other. In a simple n-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a categorical distribution. n-gram of size 1 is referred to as a unigram, size 2 is a bigram or digram, size 3 is a trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned alpha parameter: {'alpha': 1}\n",
      "Mean cross-validated accuracy of tuned model for the hold out data: 0.73992673992674\n",
      "\n",
      "Accuracy Score for Training Set (bi-gram-NB-CV): 0.9743589743589743\n",
      "Accuracy Score for Test Set (bi-gram-NB-CV): 0.717948717948718\n"
     ]
    }
   ],
   "source": [
    "# bi-gram feature Multinomial Naive Bayes + Cross Validation\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_ngram = vectorizer.fit_transform(df_ml.text_str)\n",
    "\n",
    "x_ngram_train, x_ngram_test, y_ngram_train, y_ngram_test = train_test_split(X_ngram, y, test_size=0.2, random_state=5)\n",
    "\n",
    "nb_ngram = MultinomialNB()\n",
    "alpha_space = [1, 2, 5, 10, 50]\n",
    "param_grid = {'alpha': alpha_space}\n",
    "nb_ngram_CV = GridSearchCV(nb_ngram, param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "nb_ngram_CV.fit(x_ngram_train,y_ngram_train)\n",
    "\n",
    "print(\"Tuned alpha parameter: {}\".format(nb_ngram_CV.best_params_)) \n",
    "print(\"Mean cross-validated accuracy of tuned model for the hold out data: {}\".format(nb_ngram_CV.best_score_)) \n",
    "print('')\n",
    "print('Accuracy Score for Training Set (bi-gram-NB-CV): {}'\\\n",
    "      .format(accuracy_score(y_ngram_train, nb_ngram_CV.predict(x_ngram_train))))\n",
    "print('Accuracy Score for Test Set (bi-gram-NB-CV): {}'\\\n",
    "      .format(accuracy_score(y_ngram_test, nb_ngram_CV.predict(x_ngram_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned alpha parameter: {'alpha': 1}\n",
      "Mean cross-validated accuracy of tuned model for the hold out data: 0.739010989010989\n",
      "\n",
      "Accuracy Score for Training Set (tri-gram-NB-CV): 0.9862637362637363\n",
      "Accuracy Score for Test Set (tri-gram-NB-CV): 0.717948717948718\n"
     ]
    }
   ],
   "source": [
    "# tri-gram feature Multinomial Naive Bayes + Cross Validation\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_ngram = vectorizer.fit_transform(df_ml.text_str)\n",
    "\n",
    "x_ngram_train, x_ngram_test, y_ngram_train, y_ngram_test = train_test_split(X_ngram, y, test_size=0.2, random_state=5)\n",
    "\n",
    "nb_ngram = MultinomialNB()\n",
    "alpha_space = [1, 2, 5, 10, 50]\n",
    "param_grid = {'alpha': alpha_space}\n",
    "nb_ngram_CV = GridSearchCV(nb_ngram, param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "nb_ngram_CV.fit(x_ngram_train,y_ngram_train)\n",
    "\n",
    "print(\"Tuned alpha parameter: {}\".format(nb_ngram_CV.best_params_)) \n",
    "print(\"Mean cross-validated accuracy of tuned model for the hold out data: {}\".format(nb_ngram_CV.best_score_)) \n",
    "print('')\n",
    "print('Accuracy Score for Training Set (tri-gram-NB-CV): {}'\\\n",
    "      .format(accuracy_score(y_ngram_train, nb_ngram_CV.predict(x_ngram_train))))\n",
    "print('Accuracy Score for Test Set (tri-gram-NB-CV): {}'\\\n",
    "      .format(accuracy_score(y_ngram_test, nb_ngram_CV.predict(x_ngram_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned C - Model Regularization Parameter: {'C': 5}\n",
      "Mean cross-validated accuracy of tuned model for the hold out data: 0.771978021978022\n",
      "\n",
      "Accuracy Score for Training Set (Logistic Regression-CV): 0.9871794871794872\n",
      "Accuracy Score for Test Set (Logistic Regression-CV): 0.7545787545787546\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression + Cross Validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression() # regularization: ridge\n",
    "\n",
    "# large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more.\n",
    "parameters = {'C':[ 0.1, 1, 5, 10, 20, 30, 40, 50]} \n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(Xtfidf, y, test_size=0.2, random_state=5)\n",
    "\n",
    "# GridSearchCV\n",
    "LR_cv_tfidf = GridSearchCV(clf, param_grid = parameters, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "LR_cv_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "print(\"Tuned C - Model Regularization Parameter: {}\".format(LR_cv_tfidf.best_params_))\n",
    "print(\"Mean cross-validated accuracy of tuned model for the hold out data: {}\".format(LR_cv_tfidf.best_score_)) \n",
    "print('')\n",
    "print('Accuracy Score for Training Set (Logistic Regression-CV): {}'\\\n",
    "      .format(accuracy_score(y_train_tfidf, LR_cv_tfidf.predict(X_train_tfidf))))\n",
    "print('Accuracy Score for Test Set (Logistic Regression-CV): {}'\\\n",
    "      .format(accuracy_score(y_test_tfidf, LR_cv_tfidf.predict(X_test_tfidf))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between the different models used to classify the emails:\n",
    "\n",
    "Bag of words model was used for feature extraction from text. For text classification, multinomial Naive Bayes with two different types of vectorization for bag of words (count & TF-DIF), ngram feature Naive Bayes, and Logistic Regression with TF-IDF featuring were used. Cross-validation to tune the regularization parameters was performed for all models. The accuracy score for training set for these four models varied between 92% and 99% while the accuracy score for the test set varied between 71% and 75%. Logistic Regression had the highest accuracy score for the test set with a score of 75%. \n",
    "\n",
    "This classification has 13,479 unigram features and 1,365 data points labeled into 8 categories. The number of features were much higher than data points. Naive Bayes classifiers and Logistic Regression had comparable performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "An email classifier for labeling emails of a particular employee has been built. This classifier or extended versions of it could be used to label his emails automatically. Building the classifier for the whole company would require predefined labels and data from at least a few employees but the process for building the classifier is the same.\n",
    "\n",
    "Any of the followings could help to improve the performance of the classifier: \n",
    "- Getting access to more data points and well labeled data - labels that were assigned by the user more thoughtfully\n",
    "- Expanding our stop words list\n",
    "- Stemming or lemmazation\n",
    "- Working with data in one language, not two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
